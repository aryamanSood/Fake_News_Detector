{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb473cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import re\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae3ca72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./fake-news/train.csv')\n",
    "test = pd.read_csv('./fake-news/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88de7643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20800, 5) (5200, 4)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fbbb35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id           0\n",
      "title      558\n",
      "author    1957\n",
      "text        39\n",
      "label        0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "id          0\n",
      "title     122\n",
      "author    503\n",
      "text        7\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train.isnull().sum())\n",
    "print()\n",
    "print()\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5addaaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.fillna(' ') #replace the missing(NULL) values with a blank space\n",
    "train = train.fillna(' ') \n",
    "test['total'] = test['title']+' '+test['author']+test['text']    #merge all the columns into a single column\n",
    "train['total'] = train['title']+' '+train['author']+train['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04208ed",
   "metadata": {},
   "source": [
    "## Creating WordCloud Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d0c1b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_words = ''\n",
    "fake_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "#iterate through the csv file\n",
    "for val in train[train['label']==1].total:\n",
    "    #split the value\n",
    "    tokens=val.split()\n",
    "    \n",
    "    #convert each token into lowercase\n",
    "    for i in range (len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "        \n",
    "    real_words += \" \".join(tokens)+\" \"\n",
    "    \n",
    "for val in train[train['label']==0].total:\n",
    "    \n",
    "    #split value\n",
    "    tokens = val.split()\n",
    "    \n",
    "    #convert each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        \n",
    "        tokens[i] = tokens[i].lower()\n",
    "        \n",
    "    fake_words += \" \".join(tokens)+\" \"\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d38d1d7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'B' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [37], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m wordcloud \u001b[38;5;241m=\u001b[39m WordCloud(width \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m800\u001b[39m, height \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m800\u001b[39m,\n\u001b[0;32m      2\u001b[0m                      background_color \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m                      stopwords \u001b[38;5;241m=\u001b[39m stopwords,\n\u001b[0;32m      4\u001b[0m                      min_font_size \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mgenerate(real_words)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#plot the wordcloud image\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[43mB\u001b[49m,B), facecolor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(wordcloud)\n\u001b[0;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'B' is not defined"
     ]
    }
   ],
   "source": [
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                     background_color = 'white',\n",
    "                     stopwords = stopwords,\n",
    "                     min_font_size =10).generate(real_words)\n",
    "\n",
    "#plot the wordcloud image\n",
    "plt.figure(figsize=(8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5a6303",
   "metadata": {},
   "source": [
    "## Cleaning and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45657485",
   "metadata": {},
   "source": [
    "### 1. Regex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3da2302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuations from the string\n",
    "s = \"!</> hello please$$ </>^s!!!u%%bs&&%$cri@@@be^^^&&!& </>*to@# the&&\\ cha@@@n##%^^&nel!@# %%$\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b276a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a regular expression to remove punctuation from the given string\n",
    "s = re.sub(r'[^\\w\\s]', '',s) #if any substring does not contain word(w) or space(s) then replace it with empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c009c745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hello please subscribe to the channel \n"
     ]
    }
   ],
   "source": [
    "#string with punctuation removed\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd54e91c",
   "metadata": {},
   "source": [
    "### 2. Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c8a7b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Aryaman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#download nltk data(required for tokenisation)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69b90cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'how', 'are', 'you']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"Hello how are you\") #tokenise the given string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ac03c",
   "metadata": {},
   "source": [
    "### 3. StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32174ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Aryaman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#nltk library that contains stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words) #print list of stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94f41130",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Covid-19 pandemic has impacted many countries and what it did to the economy is very stressful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ff1900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(sentence)\n",
    "words = [w for w in words if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4d7e9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Covid-19', 'pandemic', 'impacted', 'many', 'countries', 'economy', 'stressful']\n"
     ]
    }
   ],
   "source": [
    "print(words) #only those words which hold meaning and can be used to mine important information "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d741a28",
   "metadata": {},
   "source": [
    "### 4. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ccc50e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Aryaman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Aryaman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#reduces a given word to its dictionary form(E.g studying is converted to its dictionary form: study)\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "     \n",
    "input_str = \"been had done languages cities mice\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ed267f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been\n",
      "had\n",
      "done\n",
      "language\n",
      "city\n",
      "mouse\n"
     ]
    }
   ],
   "source": [
    "#tokenise the sentence\n",
    "input_str=nltk.word_tokenize(input_str)\n",
    "\n",
    "#lemmatize each word\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a7c159",
   "metadata": {},
   "source": [
    "### Application "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13cf2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "for index,row in train.iterrows():\n",
    "    filter_sentence = ''\n",
    "    \n",
    "    sentence = row['total']\n",
    "    sentence = re.sub(r'[^\\w\\s]', '',sentence) #cleaning the sentence\n",
    "    \n",
    "    words = nltk.word_tokenize(sentence) #tokenization\n",
    "    words = [w for w in words if not w in stop_words] #stopwords removal\n",
    "    \n",
    "    for word in words:\n",
    "        filter_sentence = filter_sentence+' '+str(lemmatizer.lemmatize(word)).lower()\n",
    "        \n",
    "    train.loc[index,'total'] = filter_sentence    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04197fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>house dem aide we didnt even see comeys lette...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>flynn hillary clinton big woman campus breitb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>why truth might get you fired consortiumnewsc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>15 civilians killed in single us airstrike ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>iranian woman jailed fictional unpublished st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1   \n",
       "1  Ever get the feeling your life circles the rou...      0   \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1   \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1   \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1   \n",
       "\n",
       "                                               total  \n",
       "0   house dem aide we didnt even see comeys lette...  \n",
       "1   flynn hillary clinton big woman campus breitb...  \n",
       "2   why truth might get you fired consortiumnewsc...  \n",
       "3   15 civilians killed in single us airstrike ha...  \n",
       "4   iranian woman jailed fictional unpublished st...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de4aca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[['total','label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ff0cd1",
   "metadata": {},
   "source": [
    "## Applying NLP Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "194e2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2ace716",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train['total']\n",
    "Y_train = train['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818544ee",
   "metadata": {},
   "source": [
    "## Bag-of-words/CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1b5c85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aryaman\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "corpus = {\n",
    "    \n",
    "    'This is a document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "    \n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "vectorizer = CountVectorizer() #create a vector object\n",
    "X = vectorizer.fit_transform(corpus) #convert the features to vector form by analysing each word\n",
    "print(vectorizer.get_feature_names()) #print the feature name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ee3733e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 0 0 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 2 0 1 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324a9f7",
   "metadata": {},
   "source": [
    "## TF-iDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67513564",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF is a measure of originality of a word by comparing the number of times a word appears in a doc with the number of docs the word appears in\n",
    "#max_features sends maximum number of features to be sent as an argument to the function\n",
    "def vectorize_text(features, max_features):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer( stop_words='english',\n",
    "                                 decode_error='strict' ,\n",
    "                                 analyzer='word' ,\n",
    "                                 ngram_range=(1, 2), #individual tokens and two words will be considered same\n",
    "                                 max_features=max_features\n",
    "    \n",
    "                                )\n",
    "    feature_vec=vectorizer.fit_transform(features)\n",
    "    return feature_vec.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa94fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features = vectorize_text(['hello how are you doing','hi i am doing fine'],30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71ce1678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.44943642, 0.        , 0.        , 0.6316672 , 0.6316672 ,\n",
       "        0.        , 0.        ],\n",
       "       [0.33517574, 0.47107781, 0.47107781, 0.        , 0.        ,\n",
       "        0.47107781, 0.47107781]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shows the weight of different words in the text according to their importance\n",
    "#normalises on the basis of frequency also\n",
    "tfidf_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1397a",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11ab20a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction using count vectorization and tfidf\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit_transform(X_train)\n",
    "freq_term_matrix = count_vectorizer.transform(X_train)\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(freq_term_matrix)\n",
    "tf_idf_matrix = tfidf.fit_transform(freq_term_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c2e6699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 220387)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sparse matrix to be converterd into normal array\n",
    "tf_idf_matrix.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9347e",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65c0d6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion Matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "    plt.imshow(cm,interpolation='nearest',cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks=np.arange(len(classes))\n",
    "    plt.xticks(tick_marks,classes,rotation=45)\n",
    "    plt.yticks(tick_marks,classes)\n",
    "    \n",
    "    if normalize:\n",
    "        \n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized Confusion Matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix, without normalization')\n",
    "        \n",
    "    thresh = cm.max() / 2.\n",
    "    for i,j in iter.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        \n",
    "        plt.text(j,i, cm[i,j],\n",
    "                horizontalalignment=\"center\",\n",
    "                color = \"White\" if cm[i,j]>thresh else \"black\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ec717",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d519094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_counts=count_vectorizer.transform(test['total'].values)\n",
    "test_tfidf = tfidf.transform(test_counts)\n",
    "\n",
    "#split in samples\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train, y_test = train_test_split(tf_idf_matrix, Y_train, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34baf7",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2d96752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Lasso classifier on training set: 1.00\n",
      "Accuracy of Lasso classifier on test data set: 0.98\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2493,   71],\n",
       "       [  44, 2592]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e5) # C is the regularization parameter\n",
    "logreg.fit(X_train, y_train)\n",
    "pred = logreg.predict(X_test)\n",
    "print('Accuracy of Lasso classifier on training set: {:.2f}'.format(logreg.score(X_train,y_train)))\n",
    "print('Accuracy of Lasso classifier on test data set: {:.2f}'.format(logreg.score(X_test,y_test)))\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5f6845",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9fb8fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NB classifier on training set: 0.88\n",
      "Accuracy of NB classifier on test set: 0.83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2558,    6],\n",
       "       [ 853, 1783]], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "NB= MultinomialNB()\n",
    "NB.fit(X_train,y_train)\n",
    "pred = NB.predict(X_test)\n",
    "print('Accuracy of NB classifier on training set: {:.2f}'.format(NB.score(X_train,y_train)))\n",
    "print('Accuracy of NB classifier on test set: {:.2f}'.format(NB.score(X_test,y_test)))\n",
    "\n",
    "cm = confusion_matrix(y_test,pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5c3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
